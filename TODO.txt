# ex: set ff=dos ts=2 et:
# $Id$

TODO:

 * aha, there's a fairly simple optimization we could perform that should
   noticably increase performance: limit the number of data points tested
   to (#passed + 1)

   that is, begin by only testing a single data point; once there are one or
   more functions that match it exactly, test 2 and so on. in this manner,
   we eliminate a huge amount of work and end up with the same result.

   also a similar but even easier optimization is to test the largest values
   first (if possible), as overflow/output differences will be more quickly
   noticed.

 * idea for possibly better "distance" scoring
  
    currently we use the arithmetic distance between a target output and
    a function's real output, this works well.

    however, fundamentally, this assigns relative value to bits based
    on their position; given two outputs, one differing from the target by
    a single low bit and the other by a single high bit will result in the
    "high bit" difference being considered very far away from the target.

    this makes sense if only considering arithmetic/algebraic operations,
    but not if we consider bitwise operations (rotations, xor, etc.), because
    we should treat all bits of equal importance.

    therefore, when considering bitwise operations, we should devise some
    distance scheme that counts only the number of bits differing from
    the target!

 * add support for the saving/retrieval of jobs, which will allow tasks to be
   resumed if the program crashes, and would even allow the transfer of jobs
   between machines.

    * add support for some sort of cool visualization output, even if it's silly
      people are much more likely to be interested if we can at least show them
      that something is happening

      - this would probably be a component separate from the actual crunching
      - once we keep some sort of parseable output log we can write a little
        gui applet that reads them and displays visualizations of the
          - target function/data
          - best known candidate
          - a random sampling of candidates from the most recent generations

        this shouldn't be too hard, too CPU-intensive or too complex, but
        it will get people more excited about using it.

 * define a proper interface so we can lib-ize the evolution/x86 part and can
   separate the "problem" we are trying to solve into a separate program.

 * add support for multiple input parameters

 * implement some basic guidance for the function generation:

    * create an enumeration of all possible data storage destinations

    * for each instruction in X86 record which place is read and written

    * while calculating a single function, maintain a running tally of which
      places have been written to, and only choose from the pool of possible
      functions which reads operates from one of those places.

    this will increase the cost of generating a single function, but i believe
    it will produce output of higher quality, and thus reduce the overall time
    wasted on functions which are complete garbage.

 * implement a "simplify" function. this would identify effectless or redundant
   operations, such as performing some operations more than once in a row, and
   reduce them to canonical, equivalent operations
   
     hmm not so sure about this anymore, the overhead for doing it in every
     function would be far too great; but perhaps we could still apply it to
     our results


